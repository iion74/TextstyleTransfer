{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed43ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  PyTorch ë²„ì „: 2.7.0+cu118\n",
      "ğŸš€ CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "ğŸ’» GPU ì´ë¦„: NVIDIA GeForce RTX 3090 Ti\n",
      "ğŸ“¦ CUDA ë²„ì „ (PyTorchì—ì„œ ì‚¬ìš© ì¤‘ì¸): 11.8\n",
      "ğŸ”§ cuDNN ë²„ì „: 90100\n",
      "ğŸ” cuDNN ì‚¬ìš© ê°€ëŠ¥: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"ğŸ§  PyTorch ë²„ì „:\", torch.__version__)\n",
    "print(\"ğŸš€ CUDA ì‚¬ìš© ê°€ëŠ¥:\", torch.cuda.is_available())\n",
    "print(\"ğŸ’» GPU ì´ë¦„:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"ì‚¬ìš© ë¶ˆê°€\")\n",
    "print(\"ğŸ“¦ CUDA ë²„ì „ (PyTorchì—ì„œ ì‚¬ìš© ì¤‘ì¸):\", torch.version.cuda)\n",
    "print(\"ğŸ”§ cuDNN ë²„ì „:\", torch.backends.cudnn.version())\n",
    "print(\"ğŸ” cuDNN ì‚¬ìš© ê°€ëŠ¥:\", torch.backends.cudnn.enabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da10ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CUDA ì‚¬ìš©: True\n",
      "ğŸ–¥ï¸ GPU: NVIDIA GeForce RTX 3090 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI-LJH\\AppData\\Local\\Temp\\ipykernel_22724\\236329852.py:113: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì„¤ì¹˜ í•„ìš”ì‹œ\n",
    "# pip install transformers datasets\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    ")\n",
    "\n",
    "# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"âœ… CUDA ì‚¬ìš©:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"ğŸ–¥ï¸ GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_name = \"paust/pko-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "df = pd.read_csv(r\"C:\\Users\\AI-LJH\\Desktop\\cap\\smilestyle_dataset_filtered_cleaned.tsv\", sep=\"\\t\")\n",
    "df = df.dropna(thresh=3)\n",
    "\n",
    "# âœ… í•™ìŠµ ìƒ˜í”Œ ìƒì„± í•¨ìˆ˜\n",
    "def build_samples(df):\n",
    "    samples = []\n",
    "    for _, row in df.iterrows():\n",
    "        row = row.dropna()\n",
    "        for tgt_col in row.index:\n",
    "            tgt = row[tgt_col]\n",
    "            for src_col in row.index:\n",
    "                if src_col == tgt_col:\n",
    "                    continue\n",
    "                src = row[src_col]\n",
    "                prompt = f\"Translate to {tgt_col} style: {src}\"\n",
    "                samples.append((prompt, tgt))\n",
    "    return samples\n",
    "\n",
    "samples = build_samples(df)\n",
    "\n",
    "# âœ… ì»¤ìŠ¤í…€ Dataset\n",
    "class StyleDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, max_length=128):\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.samples[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            src, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tokenizer(\n",
    "            tgt, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        inputs[\"labels\"] = labels\n",
    "        return {k: v.squeeze() for k, v in inputs.items()}\n",
    "\n",
    "# âœ… í•™ìŠµ/ê²€ì¦ ë¶„í• \n",
    "train_samples, eval_samples = train_test_split(samples, test_size=0.1, random_state=42)\n",
    "train_dataset = StyleDataset(train_samples, tokenizer)\n",
    "eval_dataset = StyleDataset(eval_samples, tokenizer)\n",
    "\n",
    "# âœ… 100 stepë§ˆë‹¤ ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥ ì½œë°±\n",
    "class PrintLossCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        current = state.global_step\n",
    "        total = state.max_steps\n",
    "        if logs is not None:\n",
    "            if \"loss\" in logs:\n",
    "                print(f\"ğŸŸ  Step {current:>5} / {total}: Train Loss = {logs['loss']:.4f}\")\n",
    "            if \"eval_loss\" in logs:\n",
    "                print(f\"ğŸŸ¢ Step {current:>5} / {total}: Eval  Loss = {logs['eval_loss']:.4f}\")\n",
    "\n",
    "\n",
    "# âœ… í•™ìŠµ ì¸ì\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5_style_fildata_finetuned\",\n",
    "    eval_strategy=\"steps\",          \n",
    "    eval_steps=1000,                      # <- 1000 ìŠ¤í…ë§ˆë‹¤ í‰ê°€\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    logging_steps=1000,\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=20,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "\n",
    "# âœ… Trainer êµ¬ì„±\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[PrintLossCallback()]\n",
    ")\n",
    "\n",
    "# âœ… í•™ìŠµ ì‹œì‘\n",
    "trainer.train()\n",
    "\n",
    "# âœ… ëª¨ë¸ ì €ì¥\n",
    "trainer.save_model(\"./t5_style_finetuned/final_model\")\n",
    "tokenizer.save_pretrained(\"./t5_style_finetuned/final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98888faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‚ï¸ [formal] ë¬¸ì–´ì²´\n",
      "ì˜¤ëŠ˜ ì•ˆìƒ‰ì´ ì•ˆ ì¢‹ì€ ê²ƒ ê°™ì•„ìš”. á…²á…²á…² ì œë°œ ì˜¤ëŠ˜ì€ ê·¸ë ‡ê²Œ ë§í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”. íšŒì˜ê°€ ê³„ì† ë˜ë©´ì„œ ì¤‘ìš”í•œ ìë£Œë“¤ì€ ë‹¤ ë“¤ê³  ë‚˜ì˜¤ë ¤ê³  í–ˆì–´ìš”. ì°¸ê³ ë¡œ ì˜¤ëŠ˜ ì•„ì¹¨ ì§€ì¹œ ëª¸ì„ ì´ëŒê³  ì§‘ì— ëŒì•„ì˜¤ë‹ˆ ë§ˆìŒì´ í‰ì˜¨í•´ì§€ëŠ” ê²ƒ ê°™ì•„ìš”. ê¸°ë¶„ì´ ì¢‹ìŠµë‹ˆë‹¤. ë‹¤ì‹œ\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ—‚ï¸ [informal] êµ¬ì–´ì²´\n",
      "ì˜¤ëŠ˜ì€ ì •ë§ í•˜ë£¨ê°€ í˜ë“œë„¤. á…²á…²á…² ì œë°œ ì˜¤ëŠ˜ì€ ê·¸ë ‡ê²Œ ë§í•˜ì§€ ë§ì•„ì¤˜. ë‚œ ì˜¤ëŠ˜ íšŒì˜ê°€ ê³„ì† ë˜ë©´ì„œ ì¤‘ìš”í•œ ìë£Œë“¤ë„ ë§ˆì € ì •ë¦¬í•´ì•¼ í–ˆì–´. ì•ˆë…•í•˜ì„¸ìš”. ê·¸ë¦¬ê³  ì¤‘ìš”í•œ ì§€ì¹œ ëª¸ì„ ì´ëŒê³  ì§‘ì— ëŒì•„ì˜¤ë‹ˆ ë§ˆìŒì´ í‰ì˜¨í•´ì§€ëŠ” ê²ƒ ê°™ì•„. ì˜¤ëœë§Œì— ëŠë¼ëŠ” ë§ˆìŒì´ì•¼.\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ—‚ï¸ [android] ì•ˆë“œë¡œì´ë“œ\n",
      "ëŒ€í™”ë°©ì‹. ì˜ëª»ë¨. ì˜¤ëŠ˜. ì¼. í˜ë“¬. ì˜ˆìƒë¨. á…²á…²á…². ì§„ì‹¬. ë°”ì¨. íšŒì˜. ê³„ì†ë¨. ì¤‘ìš”í•œ. ìë£Œ. ì²˜ë¦¬. í•„ìš”í–ˆìŒ. á„á„á„. ë§ˆë¬´ë¦¬. ì‚¬ì§„. ì•ˆë“œë¡œì´ë“œ. ì–´ì œ. ì¼í•¨. ê·¸ëŸ¬ë‚˜. ì§‘. ë³µê·€. ë§ˆìŒ. í‰ì˜¨í•´ì§. ëŠë‚Œ. ì¢‹ìŒ.\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ—‚ï¸ [enfp] enfp\n",
      "ì˜¤ëŠ˜ ì•„ì¹¨ ë‚ ì”¨ê°€ ë„ˆ~ë¬´ ì¢‹ì€ ê±° ìˆì§€?! á„’á„’ ê³ ë§ˆì›¡!á„’ íšŒì˜ê°€ ê³„ì† ë ìˆ˜ë¡ ì¤‘ìš”í•œ ìë£Œë“¤ì€ ë‹¤ ë“¤êµ¬ ë‚˜ì˜¤ë ¤ê³  í–ˆì—‰ á„’á„’ ì¤‘ìš”í•œ í˜ë“  ë§˜ì„ ë’¤ë¡œí•˜ê³  ì§‘ì— ì˜¨ ë’¤ë¡œ ë§ˆìŒì´ í‰ì˜¨í•´ì§„ ê±° ê°€íƒ€!á„’á„’\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ—‚ï¸ [gentle] ì‹ ì‚¬\n",
      "ì˜¤ëŠ˜ì€ ì¡°ê¸ˆ ë” ì •ì‹ ì—†ëŠ” í•˜ë£¨ë„¤ìš”. á„’á„’, ì˜¤ëŠ˜ì€ ì •ë§ ì‰´ í‹ˆì´ íšŒì˜ê°€ ê³„ì† ë˜ë©´ì„œ ì¤‘ìš”í•œ ìë£Œë“¤ì„ ë‹¤ ë“¤ê³  ë‚˜ì˜¤ë ¤ í–ˆìŠµë‹ˆë‹¤. á„’á„’ ê°ì‚¬í•œ ì§‘ì— ì˜¨ í›„ë¡œ ëª¸ì´ ë” í”¼ê³¤í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤. á„’á„’, ì§‘ì—\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ—‚ï¸ [halbae] í• ì•„ë²„ì§€\n",
      "ì˜¤ëŠ˜ì€.ì°¸ë§ˆë¡œ ì •ì‹ ì´ ì—†ëŠ” ë“¯ í•˜êµ¬ë¨¼.ì—íœ´.ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ íšŒì˜ê°€ ê³„ì† ë˜ë©´ì„œ.ì¤‘ìš”í•œ ìë£Œë“¤ì€ ë‹¤ ë“¤ê³  ë‚˜ì˜¤ì…¨êµ¬ë¨¼.ì•„, ì¤‘ìš”í•œ ê·¸ë ‡êµ¬ë¨¼.ê·¸ëŸ° ì‹ìœ¼ë¡œ.ì§€ì¹œ ëª¸ì„ ì´ëŒê³  ì§‘ì— ì˜¤ë‹ˆê¹Œ.ë§ˆìŒì´ í‰ì˜¨í•´\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ—‚ï¸ [king] ì™•\n",
      "ê¸ˆì¼ ë‚´ë‚´ ì •ì‹ ì´ ì—†ëŠ” ë“¯ í•˜ì˜¤. ì¡°ì–¸ ê³ ë§™ì†Œ. á„’á„’ ê¸ˆì¼ ë‚´ë‚´ ë°”ì˜êµ¬ë ¤ íšŒì˜ê°€ ê³„ì† ì´ì–´ì¡Œê³ , ì¤‘ìš”í•œ ìë£Œë“¤ë„ ë‹¤ ì •ë¦¬í•´ ì£¼ì—ˆì†Œ. á„’á„’ ë§ˆë¬´ë¦¬ ê³ ëœ ëª¸ì„ ì´ëŒê³  ì§‘ì— ëŒì•„ì˜¤ëŠ”ë° ë§ˆì¹˜ ë§ˆìŒì”¨ê°€ í‰ì˜¨í•´ì§€ëŠ” ê²ƒ ê°™ì†Œ. íˆí.\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ—‚ï¸ [sosim] ì†Œì‹¬í•œ\n",
      "ì˜¤ëŠ˜ ì•ˆìƒ‰ì´ ì•ˆì¢‹ì•„ë³´ì´ë„¤.? á…²á…² ì˜¤ëŠ˜ í•˜ë£¨ë„ ë³„ë¡œ ì•ˆì¢‹ì•˜ëŠ”ë°. á…²á…² íšŒì˜ê°€ ê³„ì† ë˜ë©´ì„œ ì¤‘ìš”í•œ ìë£Œë“¤ ë‹¤ ì •ë¦¬í•´ì•¼ ëì–´. á…²á…² ì¤‘ìš”í•œ ë°œí‘œë„ ì§€ì¹œ ëª¸ ë¶™ì¡ê³  ì§‘ì— ì˜¤ë‹ˆê¹Œ ë§ˆìŒì´ í‰ì˜¨í•´ì§€ëŠ” ê²ƒ ê°™ì•„. á…²á…² ì˜¤ëœë§Œì— ë‹¤ì‹œ\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ—‚ï¸ [translator] ë²ˆì—­ê¸°\n",
      "ê·¸ëŸ°, ì˜¤ëŠ˜ ë‹¹ì‹ ì˜ í•˜ë£¨ëŠ” êµ‰ì¥íˆ í”¼ê³¤í•œ. ê·¸ê²ƒìœ¼ë¡œ ì¶”ì¸¡í•©ë‹ˆê¹Œ? ë‚˜ëŠ” ì˜¤ëŠ˜ ì¼ì„ ì¦ê¸´ë‹¤ ì´ëŸ°, íšŒì˜ëŠ” ê³„ì† ì´ì–´ì¡Œê³ , ë‚˜ëŠ” ì¤‘ìš”í•œ ìë£Œë“¤ì„ ëª¨ë‘ ì·¨ê¸‰í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ê¹Œì§€ ê·¸ê²ƒì€ ê·¸ëŸ°, ë‚˜ëŠ” ì§€ì¹œ ëª¸ì„ ì´ëŒê³  ì§‘ì— ë“¤ì–´ê°”ì„ ë•Œ ì²˜ìŒ ëŠë‚€ ê²ƒì²˜ëŸ¼ ê¸°ë¶„ì´ ì¢‹ì•„ì§„ë‹¤.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from kss import split_sentences\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# âœ… ìŠ¤íƒ€ì¼ ëª©ë¡ ì •ì˜\n",
    "styles = [\n",
    "    'formal', 'informal', 'android', 'enfp', 'gentle',\n",
    "    'halbae', 'king', 'sosim', 'translator'\n",
    "]\n",
    "\n",
    "style_kor_map = {\n",
    "    'formal': 'ë¬¸ì–´ì²´', 'informal': 'êµ¬ì–´ì²´', 'android': 'ì•ˆë“œë¡œì´ë“œ',\n",
    "    'enfp': 'enfp', 'gentle': 'ì‹ ì‚¬', 'halbae': 'í• ì•„ë²„ì§€',\n",
    "    'king': 'ì™•', 'sosim': 'ì†Œì‹¬í•œ', 'translator': 'ë²ˆì—­ê¸°'\n",
    "}\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¡œë”©\n",
    "MODEL_PATH = r\"C:\\Users\\AI-LJH\\Desktop\\ìº¡ìŠ¤í†¤\\t5_style_finetuned\\final_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\n",
    "\n",
    "# âœ… í›„ì²˜ë¦¬ í•„í„° í•¨ìˆ˜\n",
    "def clean_generated(text):\n",
    "    text = re.sub(r'(.)\\1{3,}', r'\\1\\1', text)  # 4ì ì´ìƒ ë°˜ë³µ ì œê±°\n",
    "    text = re.sub(r'[^\\w\\sê°€-í£.,?!~\\'\\\"()\\[\\]]+', '', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    text = re.sub(r'([.,?!])\\1+', r'\\1', text)  # êµ¬ë‘ì  ë°˜ë³µ ì œê±°\n",
    "    text = re.sub(r'(\\b.+?\\b)( \\1)+', r'\\1', text)  # ë¬¸ì¥ ë°˜ë³µ ì œê±°\n",
    "    return text.strip()\n",
    "\n",
    "# âœ… ìŠ¤íƒ€ì¼ ë³€í™˜ í•¨ìˆ˜\n",
    "def style_transfer(sentence, target_style, max_length=32, num_beams=5):\n",
    "    prompt = f\"{target_style} ë§íˆ¬ë¡œ ë³€í™˜: {sentence}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=4.0,\n",
    "            no_repeat_ngram_size=5,\n",
    "            length_penalty=1.2,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return clean_generated(result)\n",
    "\n",
    "# âœ… ì „ì²´ ìŠ¤íƒ€ì¼ ë³€í™˜ ìˆ˜í–‰ í•¨ìˆ˜\n",
    "def convert_all_styles(text):\n",
    "    sentences = split_sentences(text)\n",
    "    results = {}\n",
    "\n",
    "    for style in styles:\n",
    "        styled_sentences = [style_transfer(s, style) for s in sentences]\n",
    "        results[style] = \" \".join(styled_sentences)\n",
    "\n",
    "    return results\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ë¬¸ì¥\n",
    "input_text = (\n",
    "    \"ì˜¤ëŠ˜ì€ ì •ë§ ì •ì‹ ì—†ëŠ” í•˜ë£¨ì˜€ë‹¤. íšŒì˜ê°€ ê³„ì† ì´ì–´ì¡Œê³ , ì¤‘ìš”í•œ ë³´ê³ ì„œë„ ë§ˆë¬´ë¦¬í•´ì•¼ í–ˆë‹¤. \"\n",
    "    \"ì§€ì¹œ ëª¸ì„ ì´ëŒê³  ì§‘ì— ëŒì•„ì˜¤ë‹ˆ ë¹„ë¡œì†Œ ë§ˆìŒì´ í¸ì•ˆí•´ì¡Œë‹¤.\"\n",
    ")\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "outputs = convert_all_styles(input_text)\n",
    "\n",
    "# âœ… ì¶œë ¥\n",
    "for style in styles:\n",
    "    print(f\"ğŸ—‚ï¸ [{style}] {style_kor_map[style]}\")\n",
    "    print(outputs[style])\n",
    "    print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
