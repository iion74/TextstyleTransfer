{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed43ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 PyTorch 버전: 2.7.0+cu118\n",
      "🚀 CUDA 사용 가능: True\n",
      "💻 GPU 이름: NVIDIA GeForce RTX 3090 Ti\n",
      "📦 CUDA 버전 (PyTorch에서 사용 중인): 11.8\n",
      "🔧 cuDNN 버전: 90100\n",
      "🔎 cuDNN 사용 가능: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"🧠 PyTorch 버전:\", torch.__version__)\n",
    "print(\"🚀 CUDA 사용 가능:\", torch.cuda.is_available())\n",
    "print(\"💻 GPU 이름:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"사용 불가\")\n",
    "print(\"📦 CUDA 버전 (PyTorch에서 사용 중인):\", torch.version.cuda)\n",
    "print(\"🔧 cuDNN 버전:\", torch.backends.cudnn.version())\n",
    "print(\"🔎 cuDNN 사용 가능:\", torch.backends.cudnn.enabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da10ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA 사용: True\n",
      "🖥️ GPU: NVIDIA GeForce RTX 3090 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI-LJH\\AppData\\Local\\Temp\\ipykernel_22724\\236329852.py:113: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# ✅ 설치 필요시\n",
    "# pip install transformers datasets\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    ")\n",
    "\n",
    "# ✅ 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"✅ CUDA 사용:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"🖥️ GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ✅ 모델 및 토크나이저 로드\n",
    "model_name = \"paust/pko-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ✅ 데이터셋 로드\n",
    "df = pd.read_csv(r\"C:\\Users\\AI-LJH\\Desktop\\cap\\smilestyle_dataset_filtered_cleaned.tsv\", sep=\"\\t\")\n",
    "df = df.dropna(thresh=3)\n",
    "\n",
    "# ✅ 학습 샘플 생성 함수\n",
    "def build_samples(df):\n",
    "    samples = []\n",
    "    for _, row in df.iterrows():\n",
    "        row = row.dropna()\n",
    "        for tgt_col in row.index:\n",
    "            tgt = row[tgt_col]\n",
    "            for src_col in row.index:\n",
    "                if src_col == tgt_col:\n",
    "                    continue\n",
    "                src = row[src_col]\n",
    "                prompt = f\"Translate to {tgt_col} style: {src}\"\n",
    "                samples.append((prompt, tgt))\n",
    "    return samples\n",
    "\n",
    "samples = build_samples(df)\n",
    "\n",
    "# ✅ 커스텀 Dataset\n",
    "class StyleDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, max_length=128):\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.samples[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            src, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tokenizer(\n",
    "            tgt, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        inputs[\"labels\"] = labels\n",
    "        return {k: v.squeeze() for k, v in inputs.items()}\n",
    "\n",
    "# ✅ 학습/검증 분할\n",
    "train_samples, eval_samples = train_test_split(samples, test_size=0.1, random_state=42)\n",
    "train_dataset = StyleDataset(train_samples, tokenizer)\n",
    "eval_dataset = StyleDataset(eval_samples, tokenizer)\n",
    "\n",
    "# ✅ 100 step마다 실시간 로그 출력 콜백\n",
    "class PrintLossCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        current = state.global_step\n",
    "        total = state.max_steps\n",
    "        if logs is not None:\n",
    "            if \"loss\" in logs:\n",
    "                print(f\"🟠 Step {current:>5} / {total}: Train Loss = {logs['loss']:.4f}\")\n",
    "            if \"eval_loss\" in logs:\n",
    "                print(f\"🟢 Step {current:>5} / {total}: Eval  Loss = {logs['eval_loss']:.4f}\")\n",
    "\n",
    "\n",
    "# ✅ 학습 인자\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5_style_fildata_finetuned\",\n",
    "    eval_strategy=\"steps\",          \n",
    "    eval_steps=1000,                      # <- 1000 스텝마다 평가\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    logging_steps=1000,\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=20,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ Trainer 구성\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[PrintLossCallback()]\n",
    ")\n",
    "\n",
    "# ✅ 학습 시작\n",
    "trainer.train()\n",
    "\n",
    "# ✅ 모델 저장\n",
    "trainer.save_model(\"./t5_style_finetuned/final_model\")\n",
    "tokenizer.save_pretrained(\"./t5_style_finetuned/final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98888faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗂️ [formal] 문어체\n",
      "오늘 안색이 안 좋은 것 같아요. ᅲᅲᅲ 제발 오늘은 그렇게 말하지 말아주세요. 회의가 계속 되면서 중요한 자료들은 다 들고 나오려고 했어요. 참고로 오늘 아침 지친 몸을 이끌고 집에 돌아오니 마음이 평온해지는 것 같아요. 기분이 좋습니다. 다시\n",
      "--------------------------------------------------------------------------------\n",
      "🗂️ [informal] 구어체\n",
      "오늘은 정말 하루가 힘드네. ᅲᅲᅲ 제발 오늘은 그렇게 말하지 말아줘. 난 오늘 회의가 계속 되면서 중요한 자료들도 마저 정리해야 했어. 안녕하세요. 그리고 중요한 지친 몸을 이끌고 집에 돌아오니 마음이 평온해지는 것 같아. 오랜만에 느끼는 마음이야.\n",
      "--------------------------------------------------------------------------------\n",
      "🗂️ [android] 안드로이드\n",
      "대화방식. 잘못됨. 오늘. 일. 힘듬. 예상됨. ᅲᅲᅲ. 진심. 바쁨. 회의. 계속됨. 중요한. 자료. 처리. 필요했음. ᄏᄏᄏ. 마무리. 사진. 안드로이드. 어제. 일함. 그러나. 집. 복귀. 마음. 평온해짐. 느낌. 좋음.\n",
      "--------------------------------------------------------------------------------\n",
      "🗂️ [enfp] enfp\n",
      "오늘 아침 날씨가 너~무 좋은 거 있지?! ᄒᄒ 고마웡!ᄒ 회의가 계속 될수록 중요한 자료들은 다 들구 나오려고 했엉 ᄒᄒ 중요한 힘든 맘을 뒤로하고 집에 온 뒤로 마음이 평온해진 거 가타!ᄒᄒ\n",
      "--------------------------------------------------------------------------------\n",
      "🗂️ [gentle] 신사\n",
      "오늘은 조금 더 정신없는 하루네요. ᄒᄒ, 오늘은 정말 쉴 틈이 회의가 계속 되면서 중요한 자료들을 다 들고 나오려 했습니다. ᄒᄒ 감사한 집에 온 후로 몸이 더 피곤한 것 같습니다. ᄒᄒ, 집에\n",
      "--------------------------------------------------------------------------------\n",
      "🗂️ [halbae] 할아버지\n",
      "오늘은.참마로 정신이 없는 듯 하구먼.에휴.오늘은 날씨가 회의가 계속 되면서.중요한 자료들은 다 들고 나오셨구먼.아, 중요한 그렇구먼.그런 식으로.지친 몸을 이끌고 집에 오니까.마음이 평온해\n",
      "--------------------------------------------------------------------------------\n",
      "🗂️ [king] 왕\n",
      "금일 내내 정신이 없는 듯 하오. 조언 고맙소. ᄒᄒ 금일 내내 바쁘구려 회의가 계속 이어졌고, 중요한 자료들도 다 정리해 주었소. ᄒᄒ 마무리 고된 몸을 이끌고 집에 돌아오는데 마치 마음씨가 평온해지는 것 같소. 히힝.\n",
      "--------------------------------------------------------------------------------\n",
      "🗂️ [sosim] 소심한\n",
      "오늘 안색이 안좋아보이네.? ᅲᅲ 오늘 하루도 별로 안좋았는데. ᅲᅲ 회의가 계속 되면서 중요한 자료들 다 정리해야 됐어. ᅲᅲ 중요한 발표도 지친 몸 붙잡고 집에 오니까 마음이 평온해지는 것 같아. ᅲᅲ 오랜만에 다시\n",
      "--------------------------------------------------------------------------------\n",
      "🗂️ [translator] 번역기\n",
      "그런, 오늘 당신의 하루는 굉장히 피곤한. 그것으로 추측합니까? 나는 오늘 일을 즐긴다 이런, 회의는 계속 이어졌고, 나는 중요한 자료들을 모두 취급했습니다. 마지막까지 그것은 그런, 나는 지친 몸을 이끌고 집에 들어갔을 때 처음 느낀 것처럼 기분이 좋아진다.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from kss import split_sentences\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ✅ 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ 스타일 목록 정의\n",
    "styles = [\n",
    "    'formal', 'informal', 'android', 'enfp', 'gentle',\n",
    "    'halbae', 'king', 'sosim', 'translator'\n",
    "]\n",
    "\n",
    "style_kor_map = {\n",
    "    'formal': '문어체', 'informal': '구어체', 'android': '안드로이드',\n",
    "    'enfp': 'enfp', 'gentle': '신사', 'halbae': '할아버지',\n",
    "    'king': '왕', 'sosim': '소심한', 'translator': '번역기'\n",
    "}\n",
    "\n",
    "# ✅ 모델 로딩\n",
    "MODEL_PATH = r\"C:\\Users\\AI-LJH\\Desktop\\캡스톤\\t5_style_finetuned\\final_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\n",
    "\n",
    "# ✅ 후처리 필터 함수\n",
    "def clean_generated(text):\n",
    "    text = re.sub(r'(.)\\1{3,}', r'\\1\\1', text)  # 4자 이상 반복 제거\n",
    "    text = re.sub(r'[^\\w\\s가-힣.,?!~\\'\\\"()\\[\\]]+', '', text)  # 특수문자 제거\n",
    "    text = re.sub(r'([.,?!])\\1+', r'\\1', text)  # 구두점 반복 제거\n",
    "    text = re.sub(r'(\\b.+?\\b)( \\1)+', r'\\1', text)  # 문장 반복 제거\n",
    "    return text.strip()\n",
    "\n",
    "# ✅ 스타일 변환 함수\n",
    "def style_transfer(sentence, target_style, max_length=32, num_beams=5):\n",
    "    prompt = f\"{target_style} 말투로 변환: {sentence}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=4.0,\n",
    "            no_repeat_ngram_size=5,\n",
    "            length_penalty=1.2,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return clean_generated(result)\n",
    "\n",
    "# ✅ 전체 스타일 변환 수행 함수\n",
    "def convert_all_styles(text):\n",
    "    sentences = split_sentences(text)\n",
    "    results = {}\n",
    "\n",
    "    for style in styles:\n",
    "        styled_sentences = [style_transfer(s, style) for s in sentences]\n",
    "        results[style] = \" \".join(styled_sentences)\n",
    "\n",
    "    return results\n",
    "\n",
    "# ✅ 테스트 문장\n",
    "input_text = (\n",
    "    \"오늘은 정말 정신없는 하루였다. 회의가 계속 이어졌고, 중요한 보고서도 마무리해야 했다. \"\n",
    "    \"지친 몸을 이끌고 집에 돌아오니 비로소 마음이 편안해졌다.\"\n",
    ")\n",
    "\n",
    "# ✅ 실행\n",
    "outputs = convert_all_styles(input_text)\n",
    "\n",
    "# ✅ 출력\n",
    "for style in styles:\n",
    "    print(f\"🗂️ [{style}] {style_kor_map[style]}\")\n",
    "    print(outputs[style])\n",
    "    print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
